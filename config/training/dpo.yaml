# @package training
# DPO (Direct Preference Optimization) training configuration

# Training parameters
learning_rate: 5e-6
batch_size: 2
gradient_accumulation_steps: 8
max_epochs: 5
warmup_steps: 50
weight_decay: 0.01

# DPO specific parameters
dpo:
  beta: 0.1  # Temperature parameter
  label_smoothing: 0.0
  loss_type: "sigmoid"  # or "hinge", "ipo"
  reference_free: false

# Optimization
optimizer:
  _target_: torch.optim.AdamW
  lr: ${training.learning_rate}
  weight_decay: ${training.weight_decay}

# Scheduler
scheduler:
  _target_: transformers.get_linear_schedule_with_warmup
  num_warmup_steps: ${training.warmup_steps}
  num_training_steps: 500  # Will be updated dynamically

# Checkpointing
checkpoint:
  save_every_n_epochs: 1
  save_top_k: 3
  monitor: "val_loss"
  mode: "min"
